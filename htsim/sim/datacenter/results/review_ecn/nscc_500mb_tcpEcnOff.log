topology file: htsim/sim/datacenter/topologies/leaf_spine_128_1to1.topo
traffic matrix file: htsim/sim/datacenter/connection_matrices/incast_2to1_size500MB.cm
queue size 100 packets
end_time 500000 us
random seed 13
NSCC ratio 0.5
NSCC target queue delay 5 us
NSCC qa_gate 2
NSCC path entropy 16
TCP Cubic initial cwnd 10 packets
TCP Cubic HyStart enabled
TCP Cubic fast convergence enabled
ECN enabled
TCP Cubic ECN response disabled
CSV output: htsim/sim/datacenter/results/review_ecn/nscc_500mb_tcpEcnOff.csv
Logging to htsim/sim/datacenter/results/review_ecn/nscc_500mb_tcpEcnOff.dat
Loading connection matrix from htsim/sim/datacenter/connection_matrices/incast_2to1_size500MB.cm
Nodes: 128 Connections: 2 Triggers: 0 Failures: 0
Using 128 nodes
Topology load done
Fat Tree topology (0) with 1us Src-ToR links, 1us ToR-Agg links, 0us ToR switch latency, 0us Agg switch latency for 4us diameter latency.
no_of_tor_uplinks: 128
No of nodes: 128
No of pods: 1
Hosts per pod: 128
Hosts per pod: 128
ToR switches per pod: 8
Agg switches per pod: 16
No of core switches: 0
FatTreeCfg constructor done.
Tier 0 QueueSize Down 150000 bytes
Tier 0 QueueSize Up 150000 bytes
Tier 1 QueueSize Down 150000 bytes
Tier 1 QueueSize Up 150000 bytes
Tier 2 QueueSize Down 150000 bytes
ECN thresholds: low=37500 bytes, high=145500 bytes
FatTreeTopologyCfg NCORE=0 NAGG=16 NTOR=8 NSRV=128 NPOD=1 tor_switches_per_pod=8 agg_switches_per_pod=16 tiers=2 host_per_pod=128 enabled_ecn=1 enable_ecn_on_tor_downlink=1 ecn_low=37500 ecn_high=145500 num_failed_links=0 failed_link_ratio=0.25 no_of_nodes=128 hop_latency=0 switch_latency=0 diameter_latency=4000000 diameter=4 tier=0 link_latency=1000000 switch_latencies=0 bundlesize=1 downlink_speeds=100000000000 oversub=1 radix_down=16 queue_down=150000 radix_up=16 queue_up=150000 tier=1 link_latency=1000000 switch_latencies=0 bundlesize=1 downlink_speeds=100000000000 oversub=1 radix_down=8 queue_down=150000 radix_up=0 queue_up=150000
queue_id 2 ecn_low 37500 ecn_high 145500
actual nodes 128
Network RTT: 8 us
Initializing static NSCC parameters: _reference_network_linkspeed=100000000000 _reference_network_rtt=12000000 _reference_network_bdp=150000 _target_Qdelay=5000000 _network_linkspeed=100000000000 _network_rtt=8000000 _network_bdp=100000 _qa_gate=2^2 _qa_threshold=2e+07 _scaling_factor_a=0.666667 _scaling_factor_b=0.416667 _alpha=0.000910222 _fi=13653.3 _eta=409.6 _qa_scaling=1 _gamma=0.8 _fi_scale=0.166667 _delay_alpha=0.0125 _adjust_period_threshold=8000000 _adjust_bytes_threshold=33280
Creating 1 NSCC flows and 1 TCP Cubic flows
Initialize per-instance NSCC parameters: flowid 1000000001 _base_rtt=8000000 _base_bdp=100000 _bdp=100000 _min_cwnd=4160 _maxwnd=150000 _cwnd=150000
pathcount 16
Setting flow size to 524289500
Created 1 NSCC flows and 1 TCP Cubic flows
Both protocols share the SAME network queues - they will compete for bandwidth
Starting simulation
Flow nscc_64_0 flowId 1000000001 uecSrc 0 starting at 0
Flow tcpsrc finished at 83.7313
Flow tcpsrc finished at 83.7314
Flow tcpsrc finished at 83.7316
Flow tcpsrc finished at 83.7317
Flow tcpsrc finished at 83.7318
Flow tcpsrc finished at 83.7319
Flow tcpsrc finished at 83.732
Flow tcpsrc finished at 83.7322
Flow tcpsrc finished at 83.7323
Flow tcpsrc finished at 83.7324
Flow tcpsrc finished at 83.7325
Flow tcpsrc finished at 83.7326
Flow tcpsrc finished at 83.7328
Flow tcpsrc finished at 83.7329
Flow tcpsrc finished at 83.733
Flow tcpsrc finished at 83.7331
Flow tcpsrc finished at 83.7332
Flow tcpsrc finished at 83.7334
Flow tcpsrc finished at 83.7335
Flow tcpsrc finished at 83.7336
Flow tcpsrc finished at 83.7337
Flow tcpsrc finished at 83.7338
Flow tcpsrc finished at 83.734
Flow tcpsrc finished at 83.7341
Flow tcpsrc finished at 83.7342
Flow tcpsrc finished at 83.7343
Flow tcpsrc finished at 83.7344
Flow tcpsrc finished at 83.7346
Flow tcpsrc finished at 83.7347
Flow tcpsrc finished at 83.7348
Flow tcpsrc finished at 83.7349
Flow tcpsrc finished at 83.735
Flow tcpsrc finished at 83.7352
Flow tcpsrc finished at 83.7353
Flow tcpsrc finished at 83.7354
Flow tcpsrc finished at 83.7355
Flow tcpsrc finished at 83.7356
Flow tcpsrc finished at 83.7358
Flow tcpsrc finished at 83.7359
Flow tcpsrc finished at 83.736
Flow tcpsrc finished at 83.7361
Flow tcpsrc finished at 83.7362
Flow tcpsrc finished at 83.7364
Flow tcpsrc finished at 83.7365
Flow tcpsrc finished at 83.7366
Flow tcpsrc finished at 83.7367
Flow tcpsrc finished at 83.7368
Flow tcpsrc finished at 83.737
Flow tcpsrc finished at 83.7371
Flow tcpsrc finished at 83.7372
Flow tcpsrc finished at 83.7373
Flow tcpsrc finished at 83.7374
Flow tcpsrc finished at 83.7376
Flow tcpsrc finished at 83.7377
Flow tcpsrc finished at 83.7378
Flow tcpsrc finished at 83.7379
Flow tcpsrc finished at 83.738
Flow tcpsrc finished at 83.7382
Flow tcpsrc finished at 83.7383
Flow tcpsrc finished at 83.7384
Flow tcpsrc finished at 83.7385
Flow tcpsrc finished at 83.7386
Flow tcpsrc finished at 83.7388
Flow tcpsrc finished at 83.7389
Flow tcpsrc finished at 83.739
Flow tcpsrc finished at 83.7391
Flow tcpsrc finished at 83.7392
Flow tcpsrc finished at 83.7394
Flow tcpsrc finished at 83.7395
Flow tcpsrc finished at 83.7396
Flow tcpsrc finished at 83.7397
Flow tcpsrc finished at 83.7398
Flow tcpsrc finished at 83.74
Flow tcpsrc finished at 83.7401
Flow tcpsrc finished at 83.7402
Flow tcpsrc finished at 83.7403
Flow tcpsrc finished at 83.7404
Flow tcpsrc finished at 83.7406
Flow tcpsrc finished at 83.7407
Flow tcpsrc finished at 83.7408
Flow tcpsrc finished at 83.7409
Flow tcpsrc finished at 83.741
Flow tcpsrc finished at 83.7412
Flow tcpsrc finished at 83.7413
Flow tcpsrc finished at 83.7414
Flow tcpsrc finished at 83.7415
Flow tcpsrc finished at 83.7416
Flow tcpsrc finished at 83.7418
Flow tcpsrc finished at 83.7419
Flow tcpsrc finished at 83.742
Flow tcpsrc finished at 83.7421
Flow tcpsrc finished at 83.7422
Flow tcpsrc finished at 83.7424
Flow tcpsrc finished at 83.7425
Flow tcpsrc finished at 83.7426
Flow tcpsrc finished at 83.7427
Flow tcpsrc finished at 83.7428
Flow tcpsrc finished at 83.743
Flow tcpsrc finished at 83.7431
Flow tcpsrc finished at 83.7432
Flow tcpsrc finished at 83.7433
Flow tcpsrc finished at 83.7434
Flow tcpsrc finished at 83.7436
Flow tcpsrc finished at 83.7437
Flow tcpsrc finished at 83.7438
Flow tcpsrc finished at 83.7439
Flow tcpsrc finished at 83.744
Flow tcpsrc finished at 83.7442
Flow tcpsrc finished at 83.7443
Flow tcpsrc finished at 83.7444
Flow tcpsrc finished at 83.7445
Flow tcpsrc finished at 83.7446
Flow tcpsrc finished at 83.7448
Flow nscc_64_0 flowId 1000000001 uecSrc 0 finished at 126769 total messages 1 total packets 365104 RTS 0 total bytes 524289344 in_flight now 0 fair_inc 5784 prop_inc 4818 fast_inc 89446526 eta_inc 3010240 multi_dec -373250 quick_dec -184791 nack_dec -8404500
Trigger 0 fired, 1 targets
Done at 490000 us
CSV results written to htsim/sim/datacenter/results/review_ecn/nscc_500mb_tcpEcnOff.csv

========================================
INTER-PROTOCOL FAIRNESS RESULTS
========================================

=== NSCC Statistics ===
NSCC flows completed: 1/1
NSCC total bytes received (unique): 524289344
NSCC per-flow throughput (Gbps): mean=33.0864 median=33.0864 p99=33.0864

=== TCP Cubic Statistics ===
TCP Cubic flows completed: 1/1
TCP Cubic total bytes received: 524292001
TCP Cubic retransmits: 2990
TCP Cubic per-flow throughput (Gbps): mean=50.0928 median=50.0928 p99=50.0928

=== Competitive Fairness Analysis ===
Mode: PHASE ANALYSIS (at least one flow completed)
Phase 1 (overlap): 0 - 83731.3 us (83731.3 us)
Phase 2 (solo):    83731.3 - 126769 us (43037.2 us)
Cubic finished first. NSCC ran solo for 43037.2 us
Estimated NSCC solo bytes (Phase 2): 537964995

Competitive throughput (Phase 1 only):
  NSCC:  0 bytes, 0 Gbps
  Cubic: 524292001 bytes, 50.0928 Gbps

Competitive bandwidth share:
  NSCC:  0%
  Cubic: 100%
Competitive JFI: 0.5

=== Raw Bandwidth Share (total bytes, for reference) ===
NSCC:  49.9999%
Cubic: 50.0001%

=== Jain's Fairness Index (per-flow) ===
Jain's Fairness Index (all flows): 0.959875
